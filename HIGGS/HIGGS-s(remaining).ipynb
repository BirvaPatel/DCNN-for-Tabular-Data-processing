{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Letter-Recognition #################\n",
    "# Disable info messages from the tesnorflow\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "################## Import the libraries #####################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D,MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler,scale\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.ndimage import zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3         4         5         6   \\\n",
      "8975414   1.0  1.178396 -0.566904 -0.777604  0.545251  1.228478  0.931279   \n",
      "8663710   0.0  1.457484  0.834637 -1.163233  1.345466 -1.632310  1.105608   \n",
      "184486    1.0  0.677134 -0.299063  0.848245  2.369470  1.466420  0.761073   \n",
      "2415959   1.0  0.944876 -1.166869  1.198362  1.028024 -0.829216  1.198864   \n",
      "2996681   0.0  0.535119 -0.013690  0.738382  1.253240 -0.212996  0.460326   \n",
      "...       ...       ...       ...       ...       ...       ...       ...   \n",
      "9301101   1.0  0.488634  0.835611  0.598557  0.729240  0.181285  1.266378   \n",
      "2471621   1.0  0.536949 -0.711051 -0.694930  0.832150  1.147786  1.776630   \n",
      "6148509   1.0  0.926026 -1.495096 -0.927417  0.540247  0.557798  0.905171   \n",
      "10782333  0.0  1.433327  0.100265 -0.394192  1.321262 -1.138788  0.988900   \n",
      "4982818   1.0  0.706415  0.298954 -0.721563  1.069999  0.048215  0.716094   \n",
      "\n",
      "                7         8         9   ...        19        20        21  \\\n",
      "8975414  -0.040625  0.470271  0.000000  ... -0.153700 -0.115131  0.000000   \n",
      "8663710  -0.216886 -0.024334  0.000000  ... -0.269463  0.858842  3.101961   \n",
      "184486   -0.742698 -0.279901  2.173076  ...  0.520887 -0.455292  3.101961   \n",
      "2415959   0.336653  0.131548  0.000000  ... -0.745838 -0.435870  3.101961   \n",
      "2996681  -1.128887 -1.716845  0.000000  ...  1.661022  0.180740  0.000000   \n",
      "...            ...       ...       ...  ...       ...       ...       ...   \n",
      "9301101  -0.405029 -0.695131  1.086538  ...  1.071383  0.325572  0.000000   \n",
      "2471621   0.430725 -0.068684  1.086538  ... -1.203058  1.157940  0.000000   \n",
      "6148509  -0.154501 -0.365275  2.173076  ... -1.154754  0.878819  0.000000   \n",
      "10782333  0.504992  1.117782  2.173076  ...  1.145504  0.608577  3.101961   \n",
      "4982818  -0.434736  0.857225  0.000000  ...  1.091370 -1.206091  3.101961   \n",
      "\n",
      "                22        23        24        25        26        27        28  \n",
      "8975414   0.896167  0.975962  1.079383  0.762276  0.892595  0.779965  0.736820  \n",
      "8663710   0.852262  0.944986  0.985402  0.756538  0.245856  0.644849  0.981459  \n",
      "184486    2.563522  1.506704  0.990987  1.274319  0.476154  0.985705  1.062829  \n",
      "2415959   1.336028  1.166364  1.321067  0.793884  0.929184  0.889303  0.969223  \n",
      "2996681   0.928028  0.774636  0.986476  0.784120  0.288758  0.641621  0.655370  \n",
      "...            ...       ...       ...       ...       ...       ...       ...  \n",
      "9301101   1.128368  1.333654  0.983161  2.120058  1.766686  2.020609  1.553312  \n",
      "2471621   0.837510  1.291514  0.984888  1.180755  0.901057  1.225168  1.001897  \n",
      "6148509   0.699156  0.949389  0.987180  1.074368  1.296401  0.896192  0.820768  \n",
      "10782333  0.939285  0.848219  1.183924  0.978185  0.606480  0.959959  0.851993  \n",
      "4982818   0.879983  1.099226  0.990924  0.731283  0.895170  0.921899  0.881220  \n",
      "\n",
      "[700000 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "############### Load the dataset ###################\n",
    "path = 'HIGGS.csv'\n",
    "classes = 2\n",
    "data=pd.read_csv(path, header=None)\n",
    "samples = data.sample(n=700000)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =samples.iloc[:,1:]\n",
    "Y =samples.iloc[:,0]\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "classes = 2\n",
    "Y = to_categorical(Y,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560000, 28) (560000, 2)\n",
      "(140000, 28) (140000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560000, 36) (140000, 36)\n",
      "(560000, 6, 6) (140000, 6, 6)\n"
     ]
    }
   ],
   "source": [
    "dim = X_train.shape[1]\n",
    "x = int(dim**(1/2)) +1\n",
    "f = x*x\n",
    "model = Sequential()\n",
    "model.add(Dense(f-dim,name='feature', activation='relu',input_shape=(dim,)))\n",
    "model.add(Dense(classes,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "# here, inputs and labels are same\n",
    "model.fit(X_train,Y_train,epochs=10,batch_size=1024,verbose=0)\n",
    "extract = Model(model.inputs, model.get_layer('feature').output)\n",
    "# predict whole inputs through it\n",
    "x1 = extract.predict(X_train)\n",
    "x2 = extract.predict(X_test)\n",
    "# concatenate on horizontal axis\n",
    "X1 = np.concatenate((X_train, x1), axis=1) \n",
    "X2 = np.concatenate((X_test, x2), axis=1) \n",
    "print(X1.shape,X2.shape)\n",
    "train = X1.reshape(X1.shape[0],x,x)\n",
    "test = X2.reshape(X2.shape[0],x,x)\n",
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560000, 32, 32, 3) (140000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "x_train = []\n",
    "x_test = []\n",
    "for i in range(len(train)):\n",
    "    img = Image.fromarray(train[i])\n",
    "    imgs = img.resize(size=(32, 32))\n",
    "    imgs = np.array(imgs)\n",
    "    x_train.append(np.repeat(imgs[:, :, np.newaxis], 3, axis=2))\n",
    "x_train = np.array(x_train,dtype='float32')\n",
    "for i in range(len(test)):\n",
    "    img = Image.fromarray(test[i])\n",
    "    imgs = img.resize(size=(32, 32))\n",
    "    imgs = np.array(imgs)\n",
    "    x_test.append(np.repeat(imgs[:, :, np.newaxis], 3, axis=2))\n",
    "x_test = np.array(x_test,dtype='float32')\n",
    "print(x_train.shape,x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "547/547 [==============================] - 134s 246ms/step - loss: 0.6294 - accuracy: 0.6541 - val_loss: 0.7334 - val_accuracy: 0.5522\n",
      "Epoch 2/20\n",
      "547/547 [==============================] - 121s 220ms/step - loss: 0.5751 - accuracy: 0.6969 - val_loss: 0.5779 - val_accuracy: 0.6975\n",
      "Epoch 3/20\n",
      "547/547 [==============================] - 121s 221ms/step - loss: 0.5522 - accuracy: 0.7163 - val_loss: 0.8427 - val_accuracy: 0.6687\n",
      "Epoch 4/20\n",
      "547/547 [==============================] - 121s 221ms/step - loss: 0.5384 - accuracy: 0.7257 - val_loss: 0.5677 - val_accuracy: 0.7062\n",
      "Epoch 5/20\n",
      "547/547 [==============================] - 122s 223ms/step - loss: 0.5285 - accuracy: 0.7328 - val_loss: 0.5587 - val_accuracy: 0.7189\n",
      "Epoch 6/20\n",
      "547/547 [==============================] - 122s 223ms/step - loss: 0.5196 - accuracy: 0.7378 - val_loss: 0.5431 - val_accuracy: 0.7230\n",
      "Epoch 7/20\n",
      "547/547 [==============================] - 123s 224ms/step - loss: 0.5148 - accuracy: 0.7423 - val_loss: 0.6737 - val_accuracy: 0.7201\n",
      "Epoch 8/20\n",
      "547/547 [==============================] - 122s 224ms/step - loss: 0.5088 - accuracy: 0.7462 - val_loss: 0.5485 - val_accuracy: 0.7227\n",
      "Epoch 9/20\n",
      "547/547 [==============================] - 123s 225ms/step - loss: 0.5023 - accuracy: 0.7506 - val_loss: 0.5264 - val_accuracy: 0.7353\n",
      "Epoch 10/20\n",
      "547/547 [==============================] - 123s 224ms/step - loss: 0.5044 - accuracy: 0.7500 - val_loss: 0.5272 - val_accuracy: 0.7368\n",
      "Epoch 11/20\n",
      "547/547 [==============================] - 122s 223ms/step - loss: 0.4960 - accuracy: 0.7552 - val_loss: 0.5332 - val_accuracy: 0.7270\n",
      "Epoch 12/20\n",
      "547/547 [==============================] - 122s 223ms/step - loss: 0.4858 - accuracy: 0.7615 - val_loss: 0.5523 - val_accuracy: 0.7180\n",
      "Epoch 13/20\n",
      "547/547 [==============================] - 122s 224ms/step - loss: 0.4807 - accuracy: 0.7650 - val_loss: 0.5450 - val_accuracy: 0.7319\n",
      "Epoch 14/20\n",
      "547/547 [==============================] - 123s 224ms/step - loss: 0.4857 - accuracy: 0.7624 - val_loss: 0.5093 - val_accuracy: 0.7455\n",
      "Epoch 15/20\n",
      "547/547 [==============================] - 121s 222ms/step - loss: 0.4699 - accuracy: 0.7714 - val_loss: 0.5172 - val_accuracy: 0.7413\n",
      "Epoch 16/20\n",
      "547/547 [==============================] - 119s 218ms/step - loss: 0.4578 - accuracy: 0.7794 - val_loss: 0.5199 - val_accuracy: 0.7383\n",
      "Epoch 17/20\n",
      "547/547 [==============================] - 119s 218ms/step - loss: 0.4478 - accuracy: 0.7854 - val_loss: 0.5133 - val_accuracy: 0.7444\n",
      "Epoch 18/20\n",
      "547/547 [==============================] - 120s 219ms/step - loss: 0.4354 - accuracy: 0.7931 - val_loss: 0.5331 - val_accuracy: 0.7409\n",
      "Epoch 19/20\n",
      "547/547 [==============================] - 120s 219ms/step - loss: 0.4211 - accuracy: 0.8015 - val_loss: 0.5361 - val_accuracy: 0.7393\n",
      "Epoch 20/20\n",
      "547/547 [==============================] - 120s 219ms/step - loss: 0.4062 - accuracy: 0.8097 - val_loss: 0.5586 - val_accuracy: 0.7391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26da633e640>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### Feature-extraction ###############\n",
    "# pretrained model for FE\n",
    "from keras import callbacks \n",
    "pretrained_model = keras.applications.DenseNet121(input_shape=(32,32,3), weights=None, include_top=False)\n",
    "x = Flatten()(pretrained_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "prediction = Dense(classes, activation='softmax')(x)\n",
    "model = Model(inputs=pretrained_model.input, outputs=prediction)\n",
    "# tell the model what cost and optimization method to use\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "# fit training data and validate on the testing data\n",
    "#model.fit(x_train,Y_train,batch_size=1024, epochs=20,verbose=1,validation_data=(x_test,Y_test))\n",
    "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\",  mode =\"min\", patience = 5,  restore_best_weights = True) \n",
    "history = model.fit(x_train,Y_train,batch_size=1024, epochs=20,verbose=1,validation_data=(x_test,Y_test), callbacks =[earlystopping]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4375/4375 [==============================] - 38s 9ms/step - loss: 0.5586 - accuracy: 0.7391\n",
      "17500/17500 [==============================] - 154s 9ms/step - loss: 0.3840 - accuracy: 0.8217\n",
      "training accuracy:  0.8217357397079468\n",
      "testing accuracy:  0.739128589630127\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test,Y_test)\n",
    "acc = model.evaluate(x_train,Y_train)\n",
    "print('training accuracy: ',acc[1])\n",
    "print('testing accuracy: ',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
